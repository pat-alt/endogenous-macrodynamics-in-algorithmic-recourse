[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "",
    "text": "Preface\nThis is an online auxiliary companion to our research paper: Endogenous Macrodynamics in Algorithmic Recourse."
  },
  {
    "objectID": "intro.html#data",
    "href": "intro.html#data",
    "title": "1  Introduction",
    "section": "1.1 Data",
    "text": "1.1 Data\nWe begin by generating the synthetic data for a simple binary classification problem. For illustrative purposes we will use data that is linearly separable. The chart below shows the data \\(\\mathcal{D}\\) at time zero, before any implementation of recourse.\n\nN = 1000\nxmax = 2\nX, ys = make_blobs(\n    N, 2; \n    centers=2, as_table=false, center_box=(-xmax => xmax), cluster_std=0.1\n)\nys .= ys.==2\nX = X'\nxs = Flux.unstack(X,2)\ndata = zip(xs,ys)\ncounterfactual_data = CounterfactualData(X,ys')\nplot()\nscatter!(counterfactual_data)\n\n\n\n\nFigure 1.1: Linearly separable synthetic data"
  },
  {
    "objectID": "intro.html#classifier",
    "href": "intro.html#classifier",
    "title": "1  Introduction",
    "section": "1.2 Classifier",
    "text": "1.2 Classifier\nTo model this data \\(\\mathcal{D}\\) we will use a linear classifier. In particular, as in the paper, we will build a logistic regression model in Flux.jl: a single layer with sigmoid activation.\n\nn_epochs = 100\nmodel = Chain(Dense(2,1))\nmod = FluxModel(model)\nModels.train(mod, counterfactual_data; n_epochs=n_epochs)\n\nFigure 1.2 below shows the linear separation of the two classes.\n\nplt_original = plot(mod, counterfactual_data; zoom=0, colorbar=false, title=\"(a)\")\n\n\n\n\nFigure 1.2: The baseline model: contours indicate the predicted label; dots indicate observed data points."
  },
  {
    "objectID": "intro.html#implementation-of-recourse",
    "href": "intro.html#implementation-of-recourse",
    "title": "1  Introduction",
    "section": "1.3 Implementation of Recourse",
    "text": "1.3 Implementation of Recourse"
  },
  {
    "objectID": "intro.html#generate-counterfactual",
    "href": "intro.html#generate-counterfactual",
    "title": "1  Introduction",
    "section": "1.4 Generate counterfactual",
    "text": "1.4 Generate counterfactual\n\nγ = 0.50\nμ = 0.10\nMarkdown.parse(\n    \"\"\"\n    To generate counterfactual explanations we will rely on the most generic approach. As our decision threshold we will use $(γ*100)% here. In other words, the counterfactual is considered as valid, as soon as the classifier is more convinced that it belongs to the target class (blue) than the non-target class (orange). In each round we will implement recourse for $(μ * 100)% of the individuals in the non-target class. \n    \"\"\"\n)\n\n\nopt = Flux.Adam(0.01)\ngen = GenericGenerator(;decision_threshold=γ, opt=opt)\n\nFigure 1.3 below shows the recourse outcome, which we denote here as \\(\\mathcal{D}^{\\prime}\\). The obvious observation at this point is that the resulting counterfactuals, while valid, are clearly distinguishable from the factuals that were always in the target class. This is not a new observation and nor is it entirely surprising. In fact, a lot of recent work in this field has tried to address this issue. In this work we wonder what happens when we let these sorts of dynamics play out further in practice. While the outcome in (b) is not surprising, it may be much harder to observe so clearly it in practice (when the data is more complex).\n\ncandidates = findall(ys.==0)\nchosen_individuals = rand(candidates, Int(round(μ*length(candidates))))\nX′ = copy(X)\ny′ = copy(ys)\nfactuals = select_factual(counterfactual_data,chosen_individuals)\noutcome = generate_counterfactual(factuals, 1, counterfactual_data, mod, gen; initialization=:identity)\nX′[:,chosen_individuals] = reduce(hcat, @.(selectdim(counterfactual(outcome), 3, 1)))\ny′[chosen_individuals] = reduce(vcat,@.(selectdim(counterfactual_label(outcome),3,1)))\ncounterfactual_data′ = CounterfactualData(X′,y′')\nplt_single = plot(mod,counterfactual_data′;zoom=0,colorbar=false,title=\"(b)\")\n\n\n\n\nFigure 1.3: The recourse outcome after one round."
  },
  {
    "objectID": "intro.html#retrain",
    "href": "intro.html#retrain",
    "title": "1  Introduction",
    "section": "1.5 Retrain",
    "text": "1.5 Retrain\nSuppose the agent in charge of the black-box system has provided recourse to a share of individuals leading to the outcome in Figure 1.3. In practice, models are regularly updated through retraining to account for concept drift, for example. For our experiments, we assume that the agent accepts \\(\\mathcal{D}^{\\prime}\\) as its new ground truth. To isolate the endogenous effects we are interested in here from any other effect, we further assume away any exogenous changes to the data that we might expect to occur in practice. Retraining the model on \\(\\mathcal{D}^{\\prime}\\) leads to a shift of the decision boundary in the direction of the non-target class (Figure 1.4).\n\nmod = Models.train(mod, counterfactual_data′)\nplt_single_retrained = plot(mod,counterfactual_data′;zoom=0,colorbar=false,title=\"(c)\")\n\n\n\n\nFigure 1.4: The retrained model."
  },
  {
    "objectID": "intro.html#repeat",
    "href": "intro.html#repeat",
    "title": "1  Introduction",
    "section": "1.6 Repeat",
    "text": "1.6 Repeat\nWe finally go on to repeat this process of recourse followed by model updates for multiple round. Figure 1.5 below presents the different stages of the experiment side-by-side, where panel (d) represents the outcome after ten rounds.\nAt first glance it seems that costs to individuals seeking recourse are gradually reduced as the decision boundary moves into the direction of the non-target class: they need to exert less effort to move to valid counterfactual states. The problem with this idea is, of course, that there is no free lunch. This reduction inflicts a burden on the agent in charge of the black-box: the group of individuals that is now classified as target class individuals looks entirely different from the original group.\nWhy is this a problem? Let’s, for example, that the two synthetic features accurately describe the credit worthiness of individual seeking loans, where credit-worthiness increases in the South-West direction. Non-target class individuals (orange) are denied credit, while target class individuals (blue) receive a loan. Then the population of borrowers in (d) is much more risky than in (a). Clearly, any lender (bank) aware of such dynamics would avoid them in practice. They might choose not to offer recourse in the first place, generating a cost to all individuals seeking recourse. Alternatively, they may reward first movers, but stop offering recourse after a few rounds.\nThis last point makes it clear that the implementation of recourse by one individual may generate external costs for other individuals. This notion motivates the ideas set out in the paper.\n\ni = 2\nwhile i <= 10\n    counterfactual_data′ = CounterfactualData(X′,y′')\n    candidates = findall(y′.==0)\n    chosen_individuals = rand(candidates, Int(round(μ*length(candidates))))\n    Models.train(mod, counterfactual_data′)\n    factuals = select_factual(counterfactual_data′,chosen_individuals)\n    outcome = generate_counterfactual(factuals, 1, counterfactual_data′, mod, gen; initialization=:identity)\n    X′[:,chosen_individuals] = reduce(hcat, @.(selectdim(counterfactual(outcome), 3, 1)))\n    y′[chosen_individuals] = reduce(vcat,@.(selectdim(counterfactual_label(outcome),3,1)))\n    i += 1\nend\nplt_single_repeat = plot(mod,counterfactual_data′;zoom=0,colorbar=false,title=\"(d)\")\n\n\nplt = plot(plt_original, plt_single, plt_single_retrained, plt_single_repeat, layout=(1,4), legend=false, axis=nothing, size=(600,165))\nsavefig(plt, joinpath(www_path, \"poc.png\"))\nplt\n\n\n\n\nFigure 1.5: The different stages of the experiment."
  },
  {
    "objectID": "sections/experiments/index.html#real-world",
    "href": "sections/experiments/index.html#real-world",
    "title": "3  Experimental Results",
    "section": "3.1 Real World",
    "text": "3.1 Real World\n\ngenerators = Dict(\n    :Generic=>GenericGenerator(decision_threshold=0.5),\n    :Latent=>REVISEGenerator(),\n    :Generic_conservative=>GenericGenerator(decision_threshold=0.9),\n    :Gravitational=>GravitationalGenerator(),\n    :ClapROAR=>ClapROARGenerator()\n)\n\n\nmax_obs = 2500\ndata_path = data_dir(\"real_world\")\ndata_sets = AlgorithmicRecourseDynamics.Data.load_real_world(max_obs; data_dir=data_path)\nchoices = [\n    :cal_housing, \n    :credit_default, \n    :gmsc, \n]\ndata_sets = filter(p -> p[1] in choices, data_sets)\n\n\nusing CounterfactualExplanations.DataPreprocessing: unpack\nbs = 500\nfunction data_loader(data::CounterfactualData)\n    X, y = unpack(data)\n    data = Flux.DataLoader((X,y),batchsize=bs)\n    return data\nend\nmodel_params = (batch_norm=false,n_hidden=64,n_layers=3,dropout=true,p_dropout=0.1)\n\n\nexperiments = set_up_experiments(\n    data_sets,models,generators; \n    pre_train_models=100, model_params=model_params, \n    data_loader=data_loader\n)\n\n\nn_evals = 5\nn_rounds = 50\nevaluate_every = Int(round(n_rounds/n_evals))\nn_folds = 5\nn_samples = 10000\nT = 100\ngenerative_model_params = (epochs=250, latent_dim=8)\nresults = run_experiments(\n    experiments;\n    save_path=output_path,evaluate_every=evaluate_every,n_rounds=n_rounds, n_folds=n_folds, T=T, n_samples=n_samples,\n    generative_model_params=generative_model_params\n)\nSerialization.serialize(joinpath(output_path,\"results_real_world.jls\"),results)\n\n\nusing Serialization\nresults = Serialization.deserialize(joinpath(output_path,\"results_real_world.jls\"))\n\n\nusing Images\nline_charts = Dict()\nerrorbar_charts = Dict()\nfor (data_name, res) in results\n    plt = plot(res)\n    Images.save(joinpath(www_path, \"line_chart_$(data_name).png\"), plt)\n    line_charts[data_name] = plt\n    plt = plot(res,maximum(res.output.n))\n    Images.save(joinpath(www_path, \"errorbar_chart_$(data_name).png\"), plt)\n    errorbar_charts[data_name] = plt\nend\n\n\n3.1.0.1 Bootstrap\n\nn_bootstrap = 10\ndf = run_bootstrap(results, n_bootstrap; filename=joinpath(output_path,\"bootstrap_real_world.csv\"))\n\n\n\n3.1.0.2 Chart in paper\nFigure 3.16 shows the chart that went into the paper.\n\nusing DataFrames, Statistics\nmodel_ = :FluxEnsemble\ndf = DataFrame() \nfor (key, val) in results\n    df_ = deepcopy(val.output)\n    df_.dataset .= key\n    df = vcat(df,df_)\nend\ndf = df[df.n .== maximum(df.n),:]\ndf = df[df.model .== model_,:]\nfilter!(:value => x -> !any(f -> f(x), (ismissing, isnothing, isnan)), df)\ngdf = groupby(df, [:generator, :dataset, :n, :name, :scope])\ndf_plot = combine(gdf, :value => (x -> [(mean(x),mean(x)+std(x),mean(x)-std(x))]) => [:mean, :ymax, :ymin])\ndf_plot = df_plot[[name in [:mmd, :model_performance] for name in df_plot.name],:]\ndf_plot = df_plot[.!(df_plot.name.==:mmd .&& df_plot.scope.!=:model),:]\ndf_plot = mapcols(x -> typeof(x) == Vector{Symbol} ? string.(x) : x, df_plot)\ntransform!(df_plot, :dataset => (X -> [x==\"cal_housing\" ? \"California Housing\" : x for x in X]) => :dataset)\ntransform!(df_plot, :dataset => (X -> [x==\"credit_default\" ? \"Credit Default\" : x for x in X]) => :dataset)\ntransform!(df_plot, :dataset => (X -> [x==\"gmsc\" ? \"GMSC\" : x for x in X]) => :dataset)\ntransform!(df_plot, :name => (X -> [x==\"mmd\" ? \"MMD (model)\" : x for x in X]) => :name)\ntransform!(df_plot, :name => (X -> [x==\"model_performance\" ? \"Performance\" : x for x in X]) => :name)\ntransform!(df_plot, :generator => (X -> [x==\"Generic\" ? \"Generic (γ=0.5)\" : x for x in X]) => :generator)\ntransform!(df_plot, :generator => (X -> [x==\"Generic_conservative\" ? \"Generic (γ=0.9)\" : x for x in X]) => :generator)\n\nncol = length(unique(df_plot.dataset))\nnrow = length(unique(df_plot.name))\n\nusing RCall\nscale_ = 2.0\nR\"\"\"\nlibrary(ggplot2)\nplt <- ggplot($df_plot) +\n    geom_bar(aes(x=n, y=mean, fill=generator), stat=\"identity\", alpha=0.5, position=\"dodge\") +\n    geom_pointrange( aes(x=n, y=mean, ymin=ymin, ymax=ymax, colour=generator), alpha=0.9, position=position_dodge(width=0.9), size=0.5) +\n    facet_grid(\n        rows = vars(name),\n        cols =  vars(dataset), \n        scales = \"free_y\"\n    ) +\n    labs(y = \"Value\") + \n    scale_fill_discrete(name=\"Generator:\") +\n    scale_colour_discrete(name=\"Generator:\") +\n    theme(\n        axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        legend.position=\"bottom\"\n    ) +\n    guides(fill=guide_legend(ncol=3))\ntemp_path <- file.path(tempdir(), \"plot.png\")\nggsave(temp_path,width=$ncol * $scale_,height=$nrow * $scale_ * 0.85) \n\"\"\"\n\nimg = Images.load(rcopy(R\"temp_path\"))\nImages.save(joinpath(www_path,\"paper_real_world_results.png\"), img)\n\n\n\nImages.load(joinpath(www_path,\"paper_real_world_results.png\"))\nFigure 3.17: ?(caption)"
  },
  {
    "objectID": "sections/generators/index.html",
    "href": "sections/generators/index.html",
    "title": "4  Generators",
    "section": "",
    "text": "The generators have been moved into CounterfactualExplanations.jl. For more information so the package documentation."
  }
]